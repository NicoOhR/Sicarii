Statisticians and probability theorists have historically salvaged and glued
together haphazardly from various mathematical disciplines to form their
study. Measure spaces and $\sigma$-algebras borrowed from topology,
characteristic functions were appropriated from Fourier analysis, entropy
got nicked from thermodynamics; first by information theory then from
there by statistics. The list goes on, and we haven't even mentioned
geometry, linear algebra, or number theory, the latter two comprise
a decent chunk of introductory statistics. There's maybe an interesting
thesis-sized article tracking the genealogy of this, frankly, pieced
together field. 

While the ability for the various studies of mathematics to accommodate
one another so well is one of it's more beautiful features, and the
statistician's eagerness to knead together several often unrelated
ingredients to make the $\pi$ is often their virtue, it does make the
subject incredibly hard to teach. There's a lot that is simply taken for
granted, and the rigorous construction of the daily tools is skipped
because of the amount of content that there is to cover. As a result,
I find that at least at the undergrad level, most statistics students
struggle to intuitively explain a tool like the $t$-distribution. And
that's statistics students; biologists, economists, and psychologist who
need the tools but often lack the prerequisite mathematics knowledge will
often treat statistics with a learned helplessness. This is of course
anecdotal, don't worry, I don't go around interrogating people about their
statistical intuition.

The hope of this article is to provide some mathematical meaning to the
procedure that is often stated and briefly glossed over.
