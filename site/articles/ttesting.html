<!-- post.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>the t-distribution and its consequences</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Baskervville:ital@0;1&family=Bodoni+Moda:ital,opsz,wght@0,6..96,400..900;1,6..96,400..900&family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&family=Wittgenstein:ital,wght@0,400..900;1,400..900&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="../style.css">

</head>
<body>
  <style>
  :not(pre) > code{
      background: #2b303b;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      overflow-x: auto;
  }
  pre.syntect,
  pre[class*="syntax"],
  pre.highlight,
  pre codeblock,          
  pre {
    max-width: 100%;
    overflow-x: auto;      
    overflow-y: hidden;     
    white-space: pre;        
    background: #111111
    -webkit-overflow-scrolling: touch;
  }
  </style>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="../" class="nav-title divider-link">Sicarii</a>
      </li>
    </ul>
  </nav>

  <main class="post-container">
    <div class="post-header">
      <h1 class="post-title">the t-distribution and its consequences</h1>
      <div class="post-meta">
        <div class="author">Nico OR</div>
        <div class="date">2025-09-21</div>
      </div>
    </div>

    <article class="post-content">
      <html><head></head><body><p>Statisticians and probability theorists have historically salvaged
and glued together from various mathematical disciplines to form their
study. Measure spaces and <span class="math inline">\(\sigma\)</span>-algebras borrowed from topology,
characteristic functions were appropriated from Fourier analysis,
entropy got nicked from thermodynamics; first by information theory then
from there by statistics. The list goes on, and we haven’t even
mentioned geometry, linear algebra, or number theory, the latter two
comprise a decent chunk of introductory statistics. There’s maybe an
interesting thesis-sized article tracking the genealogy of this frankly
pieced together field.</p>
<p>While the ability for the various disciplines of mathematics to
accommodate one another so well is one of maths more beautiful features,
and the statistician’s eagerness to knead together several often
unrelated ingredients to make the <span class="math inline">\(\pi\)</span> is often their virtue, it does make
the subject incredibly hard to teach. There’s a lot that is simply taken
for granted, and the rigorous construction of the daily tools is skipped
because of the amount of content that there is to cover. As a result, I
find that, at least at the undergrad level, most statistics students
struggle to intuitively explain a tool like the <span class="math inline">\(t\)</span>-distribution. And that’s statistics
students; biologists, economists, and psychologist who need the tools
but often lack the prerequisite mathematics knowledge will often treat
statistics with learned helplessness. Of course anecdotal, don’t worry,
I don’t go around interrogating people about their statistical
intuition. The hope of this article is to provide some mathematical
meaning to the procedure that is often stated and briefly glossed
over.</p>
<p>Let’s start with the problem statement, given a collection of data
points (referred to as the sample), drawn from some unknown population,
can we make any claims about the <em>characteristics</em> of the
population, from the <em>statistics</em> of the sample? As it turns out,
yeah (kinda). It’s important to recognize here that each sample <span class="math inline">\(x_i\)</span> is a realization of a random variable
<span class="math inline">\(X\)</span>; our population is <em>not</em>
random, however, it is exactly fixed. This is an important distinction
to keep in mind. We use the framing of probability to give us the
language to quantify what we don’t know about our population.</p>
<p>Let’s pick out a characteristic of our population that we’re
interested in and figure out what our sample tells us about it. For
example, what can we say about the mean of our population, denoted as
<span class="math inline">\(\mu\)</span>. A natural place to start if we
want to figure out what <span class="math inline">\(\mu\)</span> is
would be to calculate the <em>sample</em> mean, let’s denote it as <span class="math inline">\(\bar x\)</span> (another result of the variety of
statistics is that we still haven’t really come up with good notation).
Naively, we can say <span class="math display">\[ \mu \approx \bar
x\]</span> And the closer our sample gets to our population, the more
likely this is to be. But collecting data is difficult and expensive,
and we’d like something a bit more rigorous than <span class="math inline">\(\approx\)</span>, exactly <em>how likely</em> is
it for <span class="math inline">\(\mu = \bar x\)</span>?.</p>
<p>Since <span class="math inline">\(x_i\)</span> is a realization of a
random variable <span class="math inline">\(X_i\)</span>, and a random
variable scaled by a constant as well as the summation of random
variables is also a random variable, <span class="math inline">\(\bar
x\)</span> is the realization of a random variable as well, <span class="math display">\[\bar X = \frac1n\sum X_i\]</span>. To make life a
bit easier, lets assume that we know the distribution of <span class="math inline">\(X_i\)</span> and by extension of <span class="math inline">\(\bar X\)</span>. If in fact <span class="math inline">\(\bar x = \mu\)</span> then we would expect <span class="math inline">\(\bar x-\mu
= 0 \implies E(\bar X)- \mu = 0\)</span>, in turn <span class="math display">\[E(\bar X - \mu) = 0\]</span></p>
<p>We should pause here a little before continuing. What we said in the
above is that <em>if</em> <span class="math inline">\(\bar x =
\mu\)</span> then the <em>distribution</em> of <span class="math inline">\(\bar X - \mu\)</span> will have a mean of <span class="math inline">\(0\)</span>. So, if we know what distribution <span class="math inline">\(\bar X - \mu\)</span> follows, than we can test if
our <em>hypothesis</em> <span class="math inline">\(\bar x =
\mu\)</span> is correct. In fact, if we know the distribution of <span class="math inline">\(\bar X - \mu\)</span> we can evaluate exactly how
probable it is that <span class="math inline">\(\bar x = \mu\)</span>.
So… what is the distribution of <span class="math inline">\(\bar X -
\mu\)</span>?</p>
<p>Well, if we dictate that our hypothesis is true, then we already know
the mean, what about the variance? Because variance is shift-invariant,
that is, adding or subtracting by a constrant does not effect the
variance of the random variable:</p>
<p><span class="math display">\[ \operatorname{Var}(\bar X - \mu) =
\operatorname{Var}(\bar X)  \]</span></p>
<p>We can expand the averaging and do some algebra</p>
<p><span class="math display">\[
\begin{align*}
\operatorname{Var}(\frac1n\sum X_i) &amp;=
\frac1{n^2}\operatorname{Var}(\sum X_i) \\
&amp;= \frac1{n^2}[\operatorname{Var}(X_1) + \operatorname{Var}(X_2) +
... \operatorname{Var}(X_n) + \operatorname{Cov}(X_1, X_2) ...]
\end{align*}`
\]</span></p>
<p>In most cases where we apply the t-test, we like to assume that <span class="math inline">\(X_i\)</span>’s are indepdnent of each other, which
means that <span class="math inline">\(\operatorname{Cov}(X_i,
X_j) = 0, \forall i \ne j\)</span>. This is often a very reasonable
assumption, e.g.&nbsp;in a population of humans of the same age, heights can
be safely assumed to be indpendent. But we shouldn’t get too complacent,
indpendence of samples is not always the case, the obvious example is
time series data, where each new event is influenced by the last, but
even the example of heights can be dubious, think of heights in a
family, which is surely not indpendent.</p>
<p>Anyway, since each <span class="math inline">\(X_i\)</span> is
independent, it must be the case that their variances are the same, so
we get:</p>
<p><span class="math display">\[
\begin{align*}
\operatorname{Var}(\frac1n\sum X_i) &amp;=
\frac1{n^2}\sum{\operatorname{Var}(X_i)} \\
&amp;= \frac1{n^2}\cdot n \operatorname{Var}(X) \\
&amp;= \frac{\operatorname{Var}(X)}{n}
\end{align*}`
\]</span></p>
<p>You probably know that we call <span class="math inline">\(\operatorname{Var}(X) = \sigma^2\)</span>. So
finally</p>
<p><span class="math display">\[
\operatorname{Var}(\bar X - \mu) = \frac{\sigma^2}{n}
\]</span></p>
<p>We want our transformed distribution to be easy to work with, and
dividing it by it’s variance is a good way to do that, we’ll go over
what that gives us soon.</p>
<p><span class="math display">\[
\begin{align*}
\frac{n}{\sigma^2}\operatorname{Var}(\bar X - \mu) &amp;= 1 \\
\operatorname{Var}\left(\left (\sqrt{\frac{n}{\sigma^2}} \right)\bar
X - \mu\right) &amp;= 1 \end{align*} \]</span></p>
<p>So we get our final distribution, which we’ll call <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[ T = \frac{\bar X -
\mu}{\sqrt{\sigma^2/n}}\]</span></p>
<p>Effectively, we have divided our distribution by the <em>standard
error</em> of <span class="math inline">\(\bar X\)</span>. This does a
couple of things: firstly, it standardizes the distribution to have a
variance of <span class="math inline">\(1\)</span>, it also makes the
distribution <em>dimensionless</em>. When we say a distribution (or any
value) is dimensionless, we mean that it does not have any units
attached to it; they most commonly appear as a result of a division by a
measurement of the same units, leading to cancellation of the
“dimensions” of the measurements. These are confusing but critical to a
lot of modern science, especially physics, where they allow us to say
things in a very general sense, regardless of the particular
characteristics of the system. Removing the dimensions of our system
does a similar thing, we can now speak of our transformed distribution
in absolute terms, if two data sets , whose population is sampled from
the same type of distribution, fulfill the hypothesis that <span class="math inline">\(\bar x = \mu\)</span>, then they have the exact
same transformed distribution! Historically, back when computers were
not common place and calculating the CDF of a particular distribution
was time consuming, having a common distriubtion that we can reference
precomputed values of was very important.</p>
<p>However, this is not yet a t-distribution. If we assume that our data
<span class="math inline">\(X_i\)</span> is normal, than the above
transformed distribution is <em>exactly</em> a standard normal
distribution, which we often denote with a <span class="math inline">\(Z\)</span>. The t-distribution arises when we
<em>estimate</em> the variance of the population. It is much more common
that we do not know the variance of our population, <em>a priori</em>.
So, how should we estimate the variance of our population? A natural
answer would be the variation of the sample.</p>
<p><span class="math display">\[ s^2 = \frac{1}{n-1}\sum (X_i - \bar
X)^2\]</span></p>
<p>Again, we should recognize that <span class="math inline">\(s^2\)</span> is a random variable, which somewhat
complicates our transformed distribution. But have no fear, <span class="math inline">\(s^2\)</span> luckily has a predictable
distribution (after a little bit of fanengaling), we get the <span class="math inline">\(\chi^2\)</span> distribution (pronounced
cai-squared). The <span class="math inline">\(\chi^2\)</span>
distribution arises as a sum of squared <span class="math inline">\(N(0,1)\)</span> distributions. The mean of <span class="math inline">\((X_i - \bar X)^2\)</span> is <span class="math inline">\(0\)</span> under our assumption that <span class="math inline">\(\bar x = \mu\)</span>, then we scale it again by
dividing by <span class="math inline">\(\sigma^2\)</span>, finally we
move the <span class="math inline">\(n
-1\)</span> to the other side so:</p>
<p><span class="math display">\[ \frac{(n-1)s^2}{\sigma^2} =
\frac{1}{\sigma^2}\sum (X_i - \bar X)^2
\sim \chi^2_{n-1}\]</span></p>
<p>So lets swap out <span class="math inline">\(\sigma^2\)</span> for
<span class="math inline">\(s^2\)</span> in our transformed variable</p>
<p><span class="math display">\[ \frac{\bar X - \bar x}{s/\sqrt{n}}
\]</span></p>
<p>To get the denominator to look like the <span class="math inline">\(\chi^2\)</span> distribution we described, we need
to do some pretty meaningless algebra, which I have added for
completness sake</p>
<p><span class="math display">\[
\begin{align*}
T
  &amp;= \frac{\bar X - \mu}{\,s/\sqrt{n}\,} \\[4pt] &amp;= \frac{(\bar
  X - \mu)\,(\sqrt{n}/\sigma)}{(s/\sqrt{n})\,(\sqrt{n}/\sigma)} \\
&amp;=
  \frac{\displaystyle \frac{\bar X -
\mu}{\sigma/\sqrt{n}}}{\displaystyle
  s/\sigma} \\[10pt] &amp;= \frac{Z}{\,s/\sigma\,} \\[8pt] &amp;=
  \frac{Z}{\sqrt{s^2/\sigma^2}} \\[6pt] &amp;=
  \frac{Z}{\sqrt{\frac{(n-1)s^2/\sigma^2}{\,n-1\,}}}  \\ &amp;=
  \frac{Z}{\sqrt{V/n-1}}. \end{align*} \]</span></p>
<p>So we get that <span class="math inline">\(T\)</span> is the ratio of
the standard normal distribution the square root of a scaled <span class="math inline">\(\chi^2\)</span> distribution, which, finally, is
the t-distribution. A complete proof of this fact can be found on one of
my <a href="https://statproofbook.github.io/P/t-pdf.html">favorite
sites</a>.</p>
</body></html>
    </article>
  </main>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>
</html>
